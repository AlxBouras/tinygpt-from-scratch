# tinygpt-from-scratch
A minimal, readable implementation of a Transformer-based GPT-style language model.

The goal is to implement the core components of a decoder-only Transformer
and train it end-to-end on a small text corpus, with as little abstraction
as possible.

## Scope

**This project focuses on:**
- A clean implementation of a decoder-only Transformer
- Causal self-attention
- Token and positional embeddings
- Autoregressive language modeling
- Training and text generation loops

